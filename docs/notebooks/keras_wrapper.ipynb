{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using wrappers for Gensim models for working with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about using gensim models as a part of your Keras models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrappers available (as of now) are :\n",
    "* Word2Vec (uses the function ```get_embedding_layer``` defined in  ```gensim.models.keyedvectors```)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Word2Vec, we import the corresponding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dummy set of sentences to train the Word2Vec model associated with the wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['human', 'interface', 'computer'],\n",
    "    ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "    ['eps', 'user', 'interface', 'system'],\n",
    "    ['system', 'human', 'system', 'eps'],\n",
    "    ['user', 'response', 'time'],\n",
    "    ['trees'],\n",
    "    ['graph', 'trees'],\n",
    "    ['graph', 'minors', 'trees'],\n",
    "    ['graph', 'minors', 'survey']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call the wrapper and pass appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences, size=100, min_count=1, hs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use methods and atributes associated with the Word2Vec model on the model returned by the wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', 0.21846070885658264), ('eps', 0.14406150579452515), ('system', 0.12887781858444214), ('time', 0.12749384343624115), ('computer', 0.10715052485466003), ('minors', 0.08211945742368698), ('user', 0.031229231506586075), ('interface', 0.016254138201475143), ('trees', 0.005966879427433014), ('survey', -0.10215148329734802)]\n"
     ]
    }
   ],
   "source": [
    "sims = model.most_similar('graph', topn=10)   #words most similar to 'graph'\n",
    "print sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As with Word2Vec models, the results obtained after training on small input can be unexpected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Integration with Keras : Cosine Similarity Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of using the wrapper with Keras, we try to use the wrapper for word similarity task where we compute the cosine distance as a measure of similarity between the two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.engine import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would use the layer returned by the function `get_embedding_layer` in the Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_wv = model.wv\n",
    "embedding_layer = model_wv.get_embedding_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct the Keras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_a = Input(shape=(1,), dtype='int32', name='input_a')\n",
    "input_b = Input(shape=(1,), dtype='int32', name='input_b')\n",
    "embedding_a = embedding_layer(input_a)\n",
    "embedding_b = embedding_layer(input_b)\n",
    "similarity = merge([embedding_a, embedding_b], mode='cos', dot_axes=2)\n",
    "\n",
    "keras_model = Model(input=[input_a, input_b], output=similarity)\n",
    "keras_model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we input the two words which we wish to compare and retrieve the value predicted by the model as the similarity score of the two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.00596689]]]]\n"
     ]
    }
   ],
   "source": [
    "word_a = 'graph'\n",
    "word_b = 'trees'\n",
    "output = keras_model.predict([np.asarray([model.wv.vocab[word_a].index]), np.asarray([model.wv.vocab[word_b].index])])    #prob of occuring together\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Integration with Keras : 20NewsGroups Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this wrapper could be used while dealing with a real supervised task, we consider the [20NewsGroups](qwone.com/~jason/20Newsgroups/) task. Here, we take a smaller version of this data by taking a subset of the documents to be classified. First, we import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# datapath = './datasets/'\n",
    "datapath = '../../gensim/test/test_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step of the task, we iterate over the folder in which our text samples are stored, and format them into a list of samples. Also, we prepare at the same time a list of class indices matching the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = datapath + '20_newsgroup_keras/'\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we format our text samples and labels into tensors that can be fed into a neural network. To do this, we rely on Keras utilities `keras.preprocessing.text.Tokenizer` and `keras.preprocessing.sequence.pad_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, we prepare the embedding layer for which we use the wrapper as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "Keras_w2v = word2vec.Word2Vec((word2vec.LineSentence(datapath+'20_newsgroup_keras_w2v_data.txt')) ,min_count=1)\n",
    "Keras_w2v_wv = Keras_w2v.wv\n",
    "embedding_layer = Keras_w2v_wv.get_embedding_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a small 1D convnet to solve our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 1s - loss: 1.3587 - acc: 0.2857     \n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 1s - loss: 1.1048 - acc: 0.3333     \n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 1s - loss: 1.0465 - acc: 0.3810     \n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 1s - loss: 0.7774 - acc: 0.6190     \n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 1s - loss: 0.7082 - acc: 0.6667     \n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 0s - loss: 0.5722 - acc: 0.6190     \n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 0s - loss: 0.4512 - acc: 0.8571     \n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 1s - loss: 0.3029 - acc: 0.9524     \n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 1s - loss: 0.2893 - acc: 0.9048     \n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 1s - loss: 0.2487 - acc: 0.9048     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda30551e90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "# model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=1)\n",
    "model.fit(x_train, y_train, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the results above, the accuracy obtained is not that high. This is because of the small size of training data used and we could expect to obtain better accuracy for training data of larger size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
