{
 "metadata": {
  "name": "",
  "signature": "sha256:0399a84128f6ba9801e5c6a287bec7a0644784b6892896de60db48144d68daa8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "gensim doc2vec & IMDB sentiment dataset"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fetch and prep exactly as in Mikolov's go.sh shell script. (Note this cell tests for existence of required files, so steps won't repeat once the final summary file (`aclImdb/alldata-id.txt`) is available alongside this notebook.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# adapted from Mikolov's example go.sh script: \n",
      "if [ ! -f \"aclImdb/alldata-id.txt\" ]\n",
      "then\n",
      "    if [ ! -d \"aclImdb\" ] \n",
      "    then\n",
      "        if [ ! -f \"aclImdb_v1.tar.gz\" ]\n",
      "        then\n",
      "          wget --quiet http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "        fi\n",
      "      tar xf aclImdb_v1.tar.gz\n",
      "    fi\n",
      "    \n",
      "  #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
      "  function normalize_text {\n",
      "    awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
      "    -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
      "    -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
      "  }\n",
      "\n",
      "  export LC_ALL=C\n",
      "  for j in train/pos train/neg test/pos test/neg train/unsup; do\n",
      "    rm temp\n",
      "    for i in `ls aclImdb/$j`; do cat aclImdb/$j/$i >> temp; awk 'BEGIN{print;}' >> temp; done\n",
      "    normalize_text temp\n",
      "    mv temp-norm aclImdb/$j/norm.txt\n",
      "  done\n",
      "  mv aclImdb/train/pos/norm.txt aclImdb/train-pos.txt\n",
      "  mv aclImdb/train/neg/norm.txt aclImdb/train-neg.txt\n",
      "  mv aclImdb/test/pos/norm.txt aclImdb/test-pos.txt\n",
      "  mv aclImdb/test/neg/norm.txt aclImdb/test-neg.txt\n",
      "  mv aclImdb/train/unsup/norm.txt aclImdb/train-unsup.txt\n",
      "\n",
      "  cat aclImdb/train-pos.txt aclImdb/train-neg.txt aclImdb/test-pos.txt aclImdb/test-neg.txt aclImdb/train-unsup.txt > aclImdb/alldata.txt\n",
      "  awk 'BEGIN{a=0;}{print \"_*\" a \" \" $0; a++;}' < aclImdb/alldata.txt > aclImdb/alldata-id.txt\n",
      "fi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os.path\n",
      "assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data is small enough to be read into memory. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.doc2vec import TaggedDocument\n",
      "from collections import namedtuple\n",
      "\n",
      "SentimentDocument = namedtuple('SentimentDocument','words tags split sentiment')\n",
      "\n",
      "alldocs = []  # will hold all docs in original order\n",
      "with open('aclImdb/alldata-id.txt') as alldata:\n",
      "    for line_no, line in enumerate(alldata):\n",
      "        tokens = line.split()\n",
      "        words = tokens[1:]\n",
      "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
      "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
      "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
      "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
      "\n",
      "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
      "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
      "doc_list = alldocs[:]  # for reshuffling per pass\n",
      "\n",
      "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Set-up Doc2Vec Training & Evaluation Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
      "\n",
      "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
      "\n",
      "Parameter choices below vary:\n",
      "\n",
      "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
      "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
      "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
      "* added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
      "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Doc2Vec\n",
      "import gensim.models.doc2vec\n",
      "from collections import OrderedDict\n",
      "import multiprocessing\n",
      "\n",
      "cores = multiprocessing.cpu_count()\n",
      "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
      "\n",
      "simple_models = [\n",
      "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
      "    Doc2Vec(dm=1,dm_concat=1,size=100,window=5,negative=5,hs=0,min_count=2,workers=cores),\n",
      "    # PV-DBOW \n",
      "    Doc2Vec(dm=0,size=100,negative=5,hs=0,min_count=2,workers=cores),\n",
      "    # PV-DM w/average\n",
      "    Doc2Vec(dm=1,dm_mean=1,size=100,window=10,negative=5,hs=0,min_count=2,workers=cores),\n",
      "]\n",
      "\n",
      "# speed setup by sharing results of 1st model's vocabulary scan\n",
      "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
      "print(simple_models[0].compact_name)\n",
      "for model in simple_models[1:]:\n",
      "    model.reset_from(simple_models[0])\n",
      "    print(model.compact_name)\n",
      "\n",
      "models_by_name = OrderedDict((model.compact_name, model) for model in simple_models)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dmc_d100n5w5mc2t4\n",
        "dbow_d100n5mc2t4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dmm_d100n5w10mc2t4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
      "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([models_by_name['dbow_d100n5mc2t4'], models_by_name['dmm_d100n5w10mc2t4']])\n",
      "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([models_by_name['dbow_d100n5mc2t4'], models_by_name['dmc_d100n5w5mc2t4']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Predictive Evaluation Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Helper methods for evaluating error rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import statsmodels.api as sm\n",
      "from random import sample\n",
      "\n",
      "# for timing\n",
      "from contextlib import contextmanager\n",
      "from timeit import default_timer\n",
      "import time \n",
      "\n",
      "@contextmanager\n",
      "def elapsed_timer():\n",
      "    start = default_timer()\n",
      "    elapser = lambda: default_timer() - start\n",
      "    yield lambda: elapser()\n",
      "    end = default_timer()\n",
      "    elapser = lambda: end-start\n",
      "    \n",
      "def logistic_predictor_from_data(train_targets, train_regressors):\n",
      "    logit = sm.Logit(train_targets, train_regressors)\n",
      "    predictor = logit.fit(disp=0)\n",
      "    #print(predictor.summary())\n",
      "    return predictor\n",
      "\n",
      "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
      "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
      "\n",
      "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
      "    train_regressors = sm.add_constant(train_regressors)\n",
      "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
      "\n",
      "    test_data = test_set\n",
      "    if infer:\n",
      "        if infer_subsample < 1.0:\n",
      "            test_data = sample(test_data, int(infer_subsample*len(test_data)))\n",
      "        test_regressors = [test_model.infer_vector(doc.words,steps=infer_steps,alpha=infer_alpha) for doc in test_data]\n",
      "    else:\n",
      "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
      "    test_regressors = sm.add_constant(test_regressors)\n",
      "    \n",
      "    # predict & evaluate\n",
      "    test_predictions = predictor.predict(test_regressors)\n",
      "    corrects = sum(np.rint(test_predictions)==[doc.sentiment for doc in test_data])\n",
      "    errors = len(test_predictions) - corrects\n",
      "    error_rate = float(errors) / len(test_predictions)\n",
      "    return (error_rate, errors, len(test_predictions), predictor)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bulk Training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) \u2013 with added shuffling of corpus on each pass.\n",
      "\n",
      "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
      "\n",
      "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
      "\n",
      "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import shuffle\n",
      "import datetime\n",
      "\n",
      "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
      "alpha_delta = (alpha - min_alpha) / passes\n",
      "\n",
      "print(\"START %s\" % datetime.datetime.now())\n",
      "\n",
      "for epoch in range(passes):\n",
      "    shuffle(doc_list)  # shuffling gets best results\n",
      "    \n",
      "    for name, train_model in models_by_name.items():\n",
      "        # train\n",
      "        duration = 'na'\n",
      "        train_model.alpha, train_model.min_alpha = (alpha, alpha)\n",
      "        with elapsed_timer() as elapsed:\n",
      "            train_model.train(doc_list)\n",
      "            duration = '%.1f' % elapsed()\n",
      "            \n",
      "        # evaluate\n",
      "        eval_duration = ''\n",
      "        with elapsed_timer() as eval_elapsed:\n",
      "            (err, err_count, test_count, predictor) = error_rate_for_model(train_model, train_docs, test_docs)\n",
      "        eval_duration = '%.1f' % eval_elapsed()\n",
      "        if err < best_error[name]:\n",
      "            best_error[name] = err\n",
      "            print(\"%f : %i passes : %s %ss %ss\"%(err,epoch+1,name, duration, eval_duration))\n",
      "\n",
      "        eval_duration = ''\n",
      "        with elapsed_timer() as eval_elapsed:\n",
      "            (infer_err, err_count, test_count, predictor) = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
      "        eval_duration = '%.1f' % eval_elapsed()\n",
      "        if infer_err < best_error[name+'_inferred']:\n",
      "            best_error[name+'_inferred'] = infer_err\n",
      "            print(\"%f : %i passes : %s %ss %ss\"%(infer_err,epoch+1,name+'_inferred', duration, eval_duration))\n",
      "\n",
      "    print('completed pass %i at alpha %f'%(epoch+1,alpha))\n",
      "    alpha -= alpha_delta\n",
      "    \n",
      "print(\"END %s\" % str(datetime.datetime.now()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "START 2015-06-10 15:45:49.141886\n",
        "0.419480 : 1 passes : dmc_d100n5w5mc2t4 86.9s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.384000 : 1 passes : dmc_d100n5w5mc2t4_inferred 86.9s 12.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.220440 : 1 passes : dbow_d100n5mc2t4 34.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.216000 : 1 passes : dbow_d100n5mc2t4_inferred 34.1s 5.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.269200 : 1 passes : dmm_d100n5w10mc2t4 46.7s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.214800 : 1 passes : dmm_d100n5w10mc2t4_inferred 46.7s 7.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.210280 : 1 passes : dbow+dmm 0.0s 2.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.192000 : 1 passes : dbow+dmm_inferred 0.0s 14.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.219520 : 1 passes : dbow+dmc 0.0s 2.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.220000 : 1 passes : dbow+dmc_inferred 0.0s 18.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 1 at alpha 0.025000\n",
        "0.365280 : 2 passes : dmc_d100n5w5mc2t4 61.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.337200 : 2 passes : dmc_d100n5w5mc2t4_inferred 61.0s 10.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.142400 : 2 passes : dbow_d100n5mc2t4 29.3s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.159600 : 2 passes : dbow_d100n5mc2t4_inferred 29.3s 5.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.220720 : 2 passes : dmm_d100n5w10mc2t4 35.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.138480 : 2 passes : dbow+dmm 0.0s 1.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.163200 : 2 passes : dbow+dmm_inferred 0.0s 13.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.141840 : 2 passes : dbow+dmc 0.0s 2.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.172400 : 2 passes : dbow+dmc_inferred 0.0s 18.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 2 at alpha 0.023800\n",
        "0.332200 : 3 passes : dmc_d100n5w5mc2t4 61.5s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.122920 : 3 passes : dbow_d100n5mc2t4 28.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.128000 : 3 passes : dbow_d100n5mc2t4_inferred 28.9s 5.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.202280 : 3 passes : dmm_d100n5w10mc2t4 35.1s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.121240 : 3 passes : dbow+dmm 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.138400 : 3 passes : dbow+dmm_inferred 0.0s 12.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.121560 : 3 passes : dbow+dmc 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.148800 : 3 passes : dbow+dmc_inferred 0.0s 17.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 3 at alpha 0.022600\n",
        "0.312160 : 4 passes : dmc_d100n5w5mc2t4 59.4s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.322000 : 4 passes : dmc_d100n5w5mc2t4_inferred 59.4s 11.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.114560 : 4 passes : dbow_d100n5mc2t4 32.2s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.123200 : 4 passes : dbow_d100n5mc2t4_inferred 32.2s 5.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.190120 : 4 passes : dmm_d100n5w10mc2t4 41.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.214000 : 4 passes : dmm_d100n5w10mc2t4_inferred 41.9s 6.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.114680 : 4 passes : dbow+dmm 0.0s 2.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.122400 : 4 passes : dbow+dmm_inferred 0.0s 12.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.114280 : 4 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.134400 : 4 passes : dbow+dmc_inferred 0.0s 17.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 4 at alpha 0.021400\n",
        "0.290560 : 5 passes : dmc_d100n5w5mc2t4 60.4s 1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.109080 : 5 passes : dbow_d100n5mc2t4 33.6s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.108800 : 5 passes : dbow_d100n5mc2t4_inferred 33.6s 5.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-e5e0cfe67757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0melapsed_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%.1f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/scratch/Documents/dev2015/gensim_venv/src/gensim-develop/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_words, word_count, chunksize)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjob_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"putting job #%i in the queue, qsize=%i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; waiting to finish %i outstanding jobs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/scratch/miniconda3/envs/gensim_cenv/lib/python3.4/queue.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/scratch/miniconda3/envs/gensim_cenv/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Achieved Sentiment-Prediction Accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# print best error rates achieved\n",
      "errs = [(rate,name) for name, rate in best_error.items()]\n",
      "errs.sort(key=lambda pair: pair[0])\n",
      "for err in errs:\n",
      "    print(\"%f %s\"%(err[0],err[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.108800 dbow_d100n5mc2t4_inferred\n",
        "0.109080 dbow_d100n5mc2t4\n",
        "0.114280 dbow+dmc\n",
        "0.114680 dbow+dmm\n",
        "0.122400 dbow+dmm_inferred\n",
        "0.134400 dbow+dmc_inferred\n",
        "0.190120 dmm_d100n5w10mc2t4\n",
        "0.214000 dmm_d100n5w10mc2t4_inferred\n",
        "0.290560 dmc_d100n5w5mc2t4\n",
        "0.322000 dmc_d100n5w5mc2t4_inferred\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Examining Results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Are inferred vectors close to the precalculated ones?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
      "print('for doc %d...' % doc_id)\n",
      "for model in simple_models:\n",
      "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
      "    print('%s: %s' % (model.compact_name, model.docvecs.most_similar([inferred_docvec],topn=3)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "for doc 81518...\n",
        "dmc_d100n5w5mc2t4: [(81518, 0.6632639169692993), (82236, 0.5500479340553284), (11391, 0.5488752126693726)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dbow_d100n5mc2t4: [(81518, 0.9144332408905029), (61723, 0.6625540256500244), (60474, 0.6582651138305664)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dmm_d100n5w10mc2t4: [(81518, 0.8487115502357483), (61608, 0.8010116815567017), (82548, 0.8009110689163208)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated \u2013 just 3 steps starting at a high alpha \u2013 and likely need tuning for other applications.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Do close documents seem more related than distant ones?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
      "model = random.choice(simple_models)  # and a random model\n",
      "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
      "print('TARGET (%d): \u00ab%s\u00bb\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
      "print('SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model.compact_name)\n",
      "for label, index in [('MOST',0), ('MEDIAN',len(sims)//2), ('LEAST',len(sims)-1)]:\n",
      "    print('%s %s: \u00ab%s\u00bb\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TARGET (74201): \u00ablook , i have a strong stomach , but i have no use for torture porn in my entertainment . a few weeks ago , i saw a preview of this film that gave no indication of it's true nature . this , plus the intriguing poster led me to believe this was going to be a brooding drama in the hitchcock tradition . instead , i found myself watching the ugliest , most disgusting film i have ever attended of my own free will . i should have guessed when the cashier gave me a funny look while getting my ticket . the first thing i realized was the script was by an amateur . the scenes and dialog jumping around with no thought or subtlety . like other reviewers , i knew who the torturing villain was within the first few minutes . but , i still had to sit through several scenes of dismemberment and pain , which made me sick . sick that i had spent money to watch this disaster . i can't imagine why lindsay lohan would agree to be in this production . there are other venues to stretch her acting talents . neal mcdonough and julia ormond's rent must have been due . the story , such as it is has at it's core an interesting premise . a top director might have made a respectable film out of it with a total rewrite , without the gore and more atmosphere . this movie is an absolute , total disaster . no one involved has anything to be proud of .\u00bb\n",
        "\n",
        "SIMILAR/DISSIMILAR DOCS PER MODEL dbow_d100n5mc2t4:\n",
        "\n",
        "MOST (80740, 0.745945394039154): \u00abwhat a waste of film stock . overly atmospheric . dafoe and walken mailed their performances in . argento couldn't find a stamp . the dialogue seemed like improvisation , which i hope it was , because nobody should have been paid for it . even the possible saving grace of sex and nude scenes were uninspired .\u00bb\n",
        "\n",
        "MEDIAN (41578, 0.4059261083602905): \u00abawful film . terrible acting , cheesy , totally unrealistic , embarrassing to anyone who has played the game . for a start that guy is not a hooker , he would be snapped in two . as for ''i score , that's my job'' well no it's not . for the the uneducated american audience it might come across as a good film . for me , well , that's a few hours of my life i'll never get back . i read through the reviews and came across one where the guy sounded like he knew what he was talking about . then i read - ''and while american rugby may never reach the level of talent that new zealand or south africa has , third in the world is also nothing to hang your head about'' all i can say is , lmfao ! keep playing your american football and baseball , leave the real sports to the big boys .\u00bb\n",
        "\n",
        "LEAST (31799, 0.05202261731028557): \u00abok , so in any wile e . coyote-road runner cartoons , we know that wec is going to set up all sorts of traps for rr , but always maim himself in various ways . that certainly happens in \" beep , beep \" . predictable ? i guess that it is , but when you think about it , these cartoons show how the more you try to harm someone else , the more you get harmed ; sort of like how daffy duck always tries to undermine bugs bunny's integrity but bugs sees around it . overall , this is another classic from the termite terrace crowd . sometimes , i think that if we really had wanted to ease cold war tensions , we could have just let the soviet union see looney tunes cartoons ; i'm sure that they would have loved them . another great one . ps : i learned on \" jeopardy ! \" that wile e . coyote's middle name is ethelbert .\u00bb\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Do the word vectors show useful similarities?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_models = simple_models[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "# pick a random word with a suitable number of occurences\n",
      "while True:\n",
      "    word = random.choice(word_models[0].index2word)\n",
      "    if word_models[0].vocab[word].count > 10:\n",
      "        break\n",
      "# or just pick a word from the relevant domain:\n",
      "# word = 'plot'\n",
      "similars_per_model = [str(model.most_similar(word,topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
      "similar_table = (\"<table><tr><th>\" +\n",
      "    \"</th><th>\".join([model.compact_name for model in word_models]) + \n",
      "    \"</th></tr><tr><td>\" +\n",
      "    \"</td><td>\".join(similars_per_model) +\n",
      "    \"</td></tr></table>\")\n",
      "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].vocab[word].count))\n",
      "HTML(similar_table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "most similar words for 'abundance' (146 occurences)\n"
       ]
      },
      {
       "html": [
        "<table><tr><th>dmc_d100n5w5mc2t4</th><th>dbow_d100n5mc2t4</th><th>dmm_d100n5w10mc2t4</th></tr><tr><td>[('array', 0.6298288702964783),<br>\n",
        "('quantity', 0.5925834774971008),<br>\n",
        "('overabundance', 0.5884098410606384),<br>\n",
        "('meaninglessness', 0.588019073009491),<br>\n",
        "('assemblage', 0.5832504034042358),<br>\n",
        "('totality', 0.582420825958252),<br>\n",
        "('vapidity', 0.5777873992919922),<br>\n",
        "('extremity', 0.5740029811859131),<br>\n",
        "('excess', 0.5700401067733765),<br>\n",
        "('arsenal', 0.5695343017578125),<br>\n",
        "('iqs', 0.5652327537536621),<br>\n",
        "('8-9', 0.5581415891647339),<br>\n",
        "('assortment', 0.5561405420303345),<br>\n",
        "('tons', 0.5546265840530396),<br>\n",
        "('torrents', 0.5545516014099121),<br>\n",
        "('ultimatums', 0.5542378425598145),<br>\n",
        "('amount', 0.5540366172790527),<br>\n",
        "('quantities', 0.5512855052947998),<br>\n",
        "('roster', 0.5505189895629883),<br>\n",
        "('litany', 0.549481987953186)]</td><td>[('bespattered', 0.4104897975921631),<br>\n",
        "('borel', 0.39388203620910645),<br>\n",
        "(\"'devil'\", 0.3879944086074829),<br>\n",
        "('train', 0.38379138708114624),<br>\n",
        "('nagoya', 0.377510130405426),<br>\n",
        "('gencon', 0.37605035305023193),<br>\n",
        "('geometric', 0.3748994469642639),<br>\n",
        "('un-funniest', 0.3717661201953888),<br>\n",
        "('psychotherapy', 0.3682197332382202),<br>\n",
        "('casted', 0.36680951714515686),<br>\n",
        "('high-tailing', 0.3651661276817322),<br>\n",
        "(\"ensign's\", 0.35965579748153687),<br>\n",
        "('rocque', 0.3588852286338806),<br>\n",
        "('publishers', 0.3534497022628784),<br>\n",
        "('pseudo-comic', 0.35254913568496704),<br>\n",
        "('mignard', 0.3523959219455719),<br>\n",
        "('pritchert', 0.3517644703388214),<br>\n",
        "('ours', 0.3500378429889679),<br>\n",
        "('bayonne', 0.3490917384624481),<br>\n",
        "('soup\u00e7on', 0.34788060188293457)]</td><td>[('overabundance', 0.8075829744338989),<br>\n",
        "('assemblage', 0.8018349409103394),<br>\n",
        "('assortment', 0.7681021094322205),<br>\n",
        "('ounce', 0.762627363204956),<br>\n",
        "('array', 0.7543219327926636),<br>\n",
        "('excess', 0.7436821460723877),<br>\n",
        "('amalgam', 0.7408836483955383),<br>\n",
        "('unheard', 0.740603506565094),<br>\n",
        "('over-abundance', 0.7144984602928162),<br>\n",
        "('over-load', 0.7117946743965149),<br>\n",
        "('overdose', 0.7113994359970093),<br>\n",
        "('infestation', 0.7078109383583069),<br>\n",
        "('amalgamation', 0.7040037512779236),<br>\n",
        "('exemplar', 0.688831090927124),<br>\n",
        "('accumulation', 0.686922550201416),<br>\n",
        "('onslaught', 0.685982346534729),<br>\n",
        "('exhibition', 0.6785738468170166),<br>\n",
        "('arsenal', 0.6754549741744995),<br>\n",
        "('involuntary', 0.6744215488433838),<br>\n",
        "('oodles', 0.6684854626655579)]</td></tr></table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "<IPython.core.display.HTML at 0x16364d1d0>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors \u2013 they remain at their random initialized values \u2013 unless you ask with the `dbow_words=1` initialization parameter. The DBOW doc vectors train faster \u2013 and are even better on tasks like IMDB sentiment-prediction \u2013 *without* simultaneous word-training. \n",
      "\n",
      "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Are the word vectors from this dataset any good at analogies?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# assuming something like\n",
      "# https://word2vec.googlecode.com/svn/trunk/questions-words.txt \n",
      "# is in local directory\n",
      "# note: this takes many minutes\n",
      "for model in word_models:\n",
      "    sections = model.accuracy('questions-words.txt')\n",
      "    correct, incorrect = (len(sum((s['correct'] for s in sections), [])), len(sum((s['incorrect'] for s in sections),[])))\n",
      "    print('%s: %0.2f%% correct (%d of %d)' % (model.compact_name, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dmc_d100n5w5mc2t4: 23.76% correct (4758 of 20024)\n",
        "dbow_d100n5mc2t4: 0.00% correct (0 of 20024)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dmm_d100n5w10mc2t4: 26.57% correct (5320 of 20024)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even though this is a tiny, domain-specific dataset, it shows some meagher capability on the general word analogies \u2013 at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Slop"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "This cell left intentionally erroneous. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To mix the Google dataset (if locally available) into the word tests..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Word2Vec\n",
      "w2v_g100b = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
      "w2v_g100b.compact_name = 'w2v_g100b'\n",
      "word_models.append(w2v_g100b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get copious logging output from above steps..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "rootLogger = logging.getLogger()\n",
      "rootLogger.setLevel(logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To auto-reload python code while developing..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
