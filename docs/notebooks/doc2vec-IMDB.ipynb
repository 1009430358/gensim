{
 "metadata": {
  "name": "",
  "signature": "sha256:fc80920786d62c0737c8530d8458e059e9c5f1a95cfefb7c34beafd875b34aa6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "gensim doc2vec & IMDB sentiment dataset"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fetch and prep exactly as in Mikolov's go.sh shell script. (Note this cell tests for existence of required files, so steps won't repeat once the final summary file (`aclImdb/alldata-id.txt`) is available alongside this notebook.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# adapted from Mikolov's example go.sh script: \n",
      "if [ ! -f \"aclImdb/alldata-id.txt\" ]\n",
      "then\n",
      "    if [ ! -d \"aclImdb\" ] \n",
      "    then\n",
      "        if [ ! -f \"aclImdb_v1.tar.gz\" ]\n",
      "        then\n",
      "          wget --quiet http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "        fi\n",
      "      tar xf aclImdb_v1.tar.gz\n",
      "    fi\n",
      "    \n",
      "  #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
      "  function normalize_text {\n",
      "    awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
      "    -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
      "    -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
      "  }\n",
      "\n",
      "  export LC_ALL=C\n",
      "  for j in train/pos train/neg test/pos test/neg train/unsup; do\n",
      "    rm temp\n",
      "    for i in `ls aclImdb/$j`; do cat aclImdb/$j/$i >> temp; awk 'BEGIN{print;}' >> temp; done\n",
      "    normalize_text temp\n",
      "    mv temp-norm aclImdb/$j/norm.txt\n",
      "  done\n",
      "  mv aclImdb/train/pos/norm.txt aclImdb/train-pos.txt\n",
      "  mv aclImdb/train/neg/norm.txt aclImdb/train-neg.txt\n",
      "  mv aclImdb/test/pos/norm.txt aclImdb/test-pos.txt\n",
      "  mv aclImdb/test/neg/norm.txt aclImdb/test-neg.txt\n",
      "  mv aclImdb/train/unsup/norm.txt aclImdb/train-unsup.txt\n",
      "\n",
      "  cat aclImdb/train-pos.txt aclImdb/train-neg.txt aclImdb/test-pos.txt aclImdb/test-neg.txt aclImdb/train-unsup.txt > aclImdb/alldata.txt\n",
      "  awk 'BEGIN{a=0;}{print \"_*\" a \" \" $0; a++;}' < aclImdb/alldata.txt > aclImdb/alldata-id.txt\n",
      "fi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os.path\n",
      "assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data is small enough to be read into memory. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.doc2vec import TaggedDocument\n",
      "from collections import namedtuple\n",
      "\n",
      "SentimentDocument = namedtuple('SentimentDocument','words tags split sentiment')\n",
      "\n",
      "alldocs = []  # will hold all docs in original order\n",
      "with open('aclImdb/alldata-id.txt') as alldata:\n",
      "    for line_no, line in enumerate(alldata):\n",
      "        tokens = line.split()\n",
      "        words = tokens[1:]\n",
      "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
      "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
      "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
      "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
      "\n",
      "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
      "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
      "doc_list = alldocs[:]  # for reshuffling per pass\n",
      "\n",
      "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Set-up Doc2Vec Training & Evaluation Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
      "\n",
      "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
      "\n",
      "Parameter choices below vary:\n",
      "\n",
      "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
      "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
      "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
      "* added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
      "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Doc2Vec\n",
      "import gensim.models.doc2vec\n",
      "from collections import OrderedDict\n",
      "import multiprocessing\n",
      "\n",
      "cores = multiprocessing.cpu_count()\n",
      "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
      "\n",
      "simple_models = [\n",
      "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
      "    Doc2Vec(dm=1,dm_concat=1,size=100,window=5,negative=5,hs=0,min_count=2,workers=cores),\n",
      "    # PV-DBOW \n",
      "    Doc2Vec(dm=0,size=100,negative=5,hs=0,min_count=2,workers=cores),\n",
      "    # PV-DM w/average\n",
      "    Doc2Vec(dm=1,dm_mean=1,size=100,window=10,negative=5,hs=0,min_count=2,workers=cores),\n",
      "]\n",
      "\n",
      "# speed setup by sharing results of 1st model's vocabulary scan\n",
      "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
      "print(simple_models[0])\n",
      "for model in simple_models[1:]:\n",
      "    model.reset_from(simple_models[0])\n",
      "    print(model)\n",
      "\n",
      "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Doc2Vec(dm/c,d100,n5,w5,mc2,t4)\n",
        "Doc2Vec(dbow,d100,n5,mc2,t4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Doc2Vec(dm/m,d100,n5,w10,mc2,t4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
      "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
      "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Predictive Evaluation Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Helper methods for evaluating error rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import statsmodels.api as sm\n",
      "from random import sample\n",
      "\n",
      "# for timing\n",
      "from contextlib import contextmanager\n",
      "from timeit import default_timer\n",
      "import time \n",
      "\n",
      "@contextmanager\n",
      "def elapsed_timer():\n",
      "    start = default_timer()\n",
      "    elapser = lambda: default_timer() - start\n",
      "    yield lambda: elapser()\n",
      "    end = default_timer()\n",
      "    elapser = lambda: end-start\n",
      "    \n",
      "def logistic_predictor_from_data(train_targets, train_regressors):\n",
      "    logit = sm.Logit(train_targets, train_regressors)\n",
      "    predictor = logit.fit(disp=0)\n",
      "    #print(predictor.summary())\n",
      "    return predictor\n",
      "\n",
      "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
      "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
      "\n",
      "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
      "    train_regressors = sm.add_constant(train_regressors)\n",
      "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
      "\n",
      "    test_data = test_set\n",
      "    if infer:\n",
      "        if infer_subsample < 1.0:\n",
      "            test_data = sample(test_data, int(infer_subsample*len(test_data)))\n",
      "        test_regressors = [test_model.infer_vector(doc.words,steps=infer_steps,alpha=infer_alpha) for doc in test_data]\n",
      "    else:\n",
      "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
      "    test_regressors = sm.add_constant(test_regressors)\n",
      "    \n",
      "    # predict & evaluate\n",
      "    test_predictions = predictor.predict(test_regressors)\n",
      "    corrects = sum(np.rint(test_predictions)==[doc.sentiment for doc in test_data])\n",
      "    errors = len(test_predictions) - corrects\n",
      "    error_rate = float(errors) / len(test_predictions)\n",
      "    return (error_rate, errors, len(test_predictions), predictor)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bulk Training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) \u2013 with added shuffling of corpus on each pass.\n",
      "\n",
      "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
      "\n",
      "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
      "\n",
      "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import shuffle\n",
      "import datetime\n",
      "\n",
      "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
      "alpha_delta = (alpha - min_alpha) / passes\n",
      "\n",
      "print(\"START %s\" % datetime.datetime.now())\n",
      "\n",
      "for epoch in range(passes):\n",
      "    shuffle(doc_list)  # shuffling gets best results\n",
      "    \n",
      "    for name, train_model in models_by_name.items():\n",
      "        # train\n",
      "        duration = 'na'\n",
      "        train_model.alpha, train_model.min_alpha = (alpha, alpha)\n",
      "        with elapsed_timer() as elapsed:\n",
      "            train_model.train(doc_list)\n",
      "            duration = '%.1f' % elapsed()\n",
      "            \n",
      "        # evaluate\n",
      "        eval_duration = ''\n",
      "        with elapsed_timer() as eval_elapsed:\n",
      "            (err, err_count, test_count, predictor) = error_rate_for_model(train_model, train_docs, test_docs)\n",
      "        eval_duration = '%.1f' % eval_elapsed()\n",
      "        best_indicator = ' '\n",
      "        if err <= best_error[name]:\n",
      "            best_error[name] = err\n",
      "            best_indicator = '*' \n",
      "        print(\"%s%f : %i passes : %s %ss %ss\"%(best_indicator,err,epoch+1,name, duration, eval_duration))\n",
      "\n",
      "        if epoch == 0 or (epoch % 5) == 0:\n",
      "            eval_duration = ''\n",
      "            with elapsed_timer() as eval_elapsed:\n",
      "                (infer_err, err_count, test_count, predictor) = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
      "            eval_duration = '%.1f' % eval_elapsed()\n",
      "            best_indicator = ' '\n",
      "            if infer_err < best_error[name+'_inferred']:\n",
      "                best_error[name+'_inferred'] = infer_err\n",
      "                best_indicator = '*'\n",
      "            print(\"%s%f : %i passes : %s %ss %ss\"%(best_indicator,infer_err,epoch+1,name+'_inferred', duration, eval_duration))\n",
      "\n",
      "    print('completed pass %i at alpha %f'%(epoch+1,alpha))\n",
      "    alpha -= alpha_delta\n",
      "    \n",
      "print(\"END %s\" % str(datetime.datetime.now()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "START 2015-06-15 23:42:20.225229\n",
        "*0.412640 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 58.4s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.401200 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 58.4s 10.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.218280 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.195600 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,t4)_inferred 28.0s 5.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.273280 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.5s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.214000 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4)_inferred 34.5s 6.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.204680 : 1 passes : dbow+dmm 0.0s 2.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.182400 : 1 passes : dbow+dmm_inferred 0.0s 11.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.216240 : 1 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.222000 : 1 passes : dbow+dmc_inferred 0.0s 16.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 1 at alpha 0.025000\n",
        "*0.358040 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 57.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.140320 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.0s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.223920 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 35.0s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.136520 : 2 passes : dbow+dmm 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.141120 : 2 passes : dbow+dmc 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 2 at alpha 0.023800\n",
        "*0.325440 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 62.6s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.124000 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.198680 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.2s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.121760 : 3 passes : dbow+dmm 0.0s 2.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.125120 : 3 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 3 at alpha 0.022600\n",
        "*0.300600 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 54.0s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.115760 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.188680 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 35.6s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.114760 : 4 passes : dbow+dmm 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.115440 : 4 passes : dbow+dmc 0.0s 2.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 4 at alpha 0.021400\n",
        "*0.281360 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 55.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.112000 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.4s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.182360 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.5s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.111800 : 5 passes : dbow+dmm 0.0s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.111560 : 5 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 5 at alpha 0.020200\n",
        "*0.266200 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 54.7s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.272000 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 54.7s 11.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.110560 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 29.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.112800 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,t4)_inferred 29.1s 5.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.178520 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 33.7s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.199200 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4)_inferred 33.7s 6.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.109040 : 6 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.118800 : 6 passes : dbow+dmm_inferred 0.0s 12.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.110400 : 6 passes : dbow+dmc 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.118400 : 6 passes : dbow+dmc_inferred 0.0s 16.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 6 at alpha 0.019000\n",
        "*0.254600 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 54.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.107920 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 27.5s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.175560 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.0s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.107880 : 7 passes : dbow+dmm 0.0s 2.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.107760 : 7 passes : dbow+dmc 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 7 at alpha 0.017800\n",
        "*0.246160 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 54.5s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.106640 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.173720 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.3s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.106640 : 8 passes : dbow+dmm 0.0s 2.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.106320 : 8 passes : dbow+dmc 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 8 at alpha 0.016600\n",
        "*0.239160 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 55.4s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.104120 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 27.8s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.170400 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.3s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.103840 : 9 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.104920 : 9 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 9 at alpha 0.015400\n",
        "*0.233320 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 53.4s 1.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.104120 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.170000 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.2s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.104080 : 10 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.104600 : 10 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 10 at alpha 0.014200\n",
        "*0.228680 : 11 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 52.8s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.222400 : 11 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 52.8s 10.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.103280 : 11 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.1s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.106000 : 11 passes : Doc2Vec(dbow,d100,n5,mc2,t4)_inferred 28.1s 5.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.167280 : 11 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.3s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.206800 : 11 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4)_inferred 34.3s 6.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.101800 : 11 passes : dbow+dmm 0.0s 2.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.115600 : 11 passes : dbow+dmm_inferred 0.0s 12.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.102920 : 11 passes : dbow+dmc 0.0s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.100400 : 11 passes : dbow+dmc_inferred 0.0s 15.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 11 at alpha 0.013000\n",
        "*0.225600 : 12 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 54.3s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.104040 : 12 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.165160 : 12 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102720 : 12 passes : dbow+dmm 0.0s 2.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.103360 : 12 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 12 at alpha 0.011800\n",
        "*0.223720 : 13 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 51.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.103520 : 13 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.4s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.165320 : 13 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 35.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102080 : 13 passes : dbow+dmm 0.0s 2.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.102480 : 13 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 13 at alpha 0.010600\n",
        "*0.221680 : 14 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 54.8s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.102440 : 14 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.5s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.164480 : 14 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 40.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102120 : 14 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.103640 : 14 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 14 at alpha 0.009400\n",
        "*0.220560 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 52.6s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.102040 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 29.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.163160 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 37.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102160 : 15 passes : dbow+dmm 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102880 : 15 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 15 at alpha 0.008200\n",
        "*0.218400 : 16 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 56.1s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.233600 : 16 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 56.1s 11.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102840 : 16 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.0s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.106000 : 16 passes : Doc2Vec(dbow,d100,n5,mc2,t4)_inferred 28.0s 5.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.161920 : 16 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.3s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.190800 : 16 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4)_inferred 34.3s 6.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102280 : 16 passes : dbow+dmm 0.0s 1.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.109200 : 16 passes : dbow+dmm_inferred 0.0s 12.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102840 : 16 passes : dbow+dmc 0.0s 2.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.114800 : 16 passes : dbow+dmc_inferred 0.0s 15.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 16 at alpha 0.007000\n",
        " 0.219000 : 17 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 53.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102960 : 17 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 27.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.161480 : 17 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.8s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102120 : 17 passes : dbow+dmm 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.102040 : 17 passes : dbow+dmc 0.0s 2.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 17 at alpha 0.005800\n",
        " 0.219600 : 18 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 53.1s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102400 : 18 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 29.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.161680 : 18 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 35.8s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.101680 : 18 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102120 : 18 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 18 at alpha 0.004600\n",
        " 0.218920 : 19 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 51.5s 1.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102320 : 19 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 28.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.161600 : 19 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.2s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.101640 : 19 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102160 : 19 passes : dbow+dmc 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 19 at alpha 0.003400\n",
        "*0.218200 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4) 51.2s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102560 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,t4) 27.9s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.161360 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,t4) 34.6s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "*0.101360 : 20 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " 0.102560 : 20 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 20 at alpha 0.002200\n",
        "END 2015-06-16 00:27:00.604456\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Achieved Sentiment-Prediction Accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# print best error rates achieved\n",
      "errs = [(rate,name) for name, rate in best_error.items()]\n",
      "errs.sort(key=lambda pair: pair[0])\n",
      "for err in errs:\n",
      "    print(\"%f %s\"%(err[0],err[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.100400 dbow+dmc_inferred\n",
        "0.101360 dbow+dmm\n",
        "0.102040 Doc2Vec(dbow,d100,n5,mc2,t4)\n",
        "0.102040 dbow+dmc\n",
        "0.106000 Doc2Vec(dbow,d100,n5,mc2,t4)_inferred\n",
        "0.109200 dbow+dmm_inferred\n",
        "0.161360 Doc2Vec(dm/m,d100,n5,w10,mc2,t4)\n",
        "0.190800 Doc2Vec(dm/m,d100,n5,w10,mc2,t4)_inferred\n",
        "0.218200 Doc2Vec(dm/c,d100,n5,w5,mc2,t4)\n",
        "0.222400 Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Examining Results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Are inferred vectors close to the precalculated ones?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
      "print('for doc %d...' % doc_id)\n",
      "for model in simple_models:\n",
      "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
      "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec],topn=3)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "for doc 10937...\n",
        "Doc2Vec(dm/c,d100,n5,w5,mc2,t4):\n",
        " [(10937, 0.6842625141143799), (7308, 0.42190566658973694), (10839, 0.4074726700782776)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Doc2Vec(dbow,d100,n5,mc2,t4):\n",
        " [(10937, 0.9522888660430908), (12203, 0.5845203399658203), (35262, 0.575614869594574)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Doc2Vec(dm/m,d100,n5,w10,mc2,t4):\n",
        " [(10937, 0.8651494979858398), (11717, 0.8156246542930603), (58074, 0.8120745420455933)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated \u2013 just 3 steps starting at a high alpha \u2013 and likely need tuning for other applications.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Do close documents seem more related than distant ones?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
      "model = random.choice(simple_models)  # and a random model\n",
      "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
      "print('TARGET (%d): \u00ab%s\u00bb\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
      "print('SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
      "for label, index in [('MOST',0), ('MEDIAN',len(sims)//2), ('LEAST',len(sims)-1)]:\n",
      "    print('%s %s: \u00ab%s\u00bb\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TARGET (90609): \u00absomehow in line with \" calendar girls \" and \" mrs henderson presents \" as it deals with the sex life of elderly ladies , \" irina palm \" is the story of the slightly-more-than-middle-aged maggie who has to raise a large sum of money in order to save her grandchild from dying , takes a job as a w*nker in a sex club ( minimal physical touch , no nude scenes , all done in the best taste ) ( . . . imaginable under the circumstances , that is ) - and finds that she has a rare talent for just that sort of work . i liked it . the story is given every conceivable , foreseeable twist and turn - a romance with the sex bar proprietor who just had to sample her talent on the sly ; her friends who are dying to be let in on the particulars , but still are too prudish not to turn their backs on her ; her son finding out and flying into a rage , and the reconciliation with her hostile daughter-in-law when she learns about maggie's sacrifice - all predictable , but still : i liked it . perhaps because everybody in the film puts out great performance . miki manoljovic is very good as the sex bar owner who falls in love with his unlikely ace employee , kevin bishop is frighteningly good as the loving , mild-mannered son who cannot really see his way through to understand his mother ( which son could , given her line of work ? ) , and marianne faithful , that rarely seen blast from the past ( my past , at least ) is certainly a far cry from her ophelia in 1969 ( yes , i do know that she's been doing bits and bobs in between , but somehow i've missed them ) . marianne faithful's slow , slightly hazy style is recognizable still , and i'd say she carries this film through in a very touching way - no pun intended .\u00bb\n",
        "\n",
        "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w10,mc2,t4):\n",
        "\n",
        "MOST (36095, 0.6738423109054565): \u00abwalter matthau is wonderful as the \" philandering \" dentist dr . julian winston whose frequent fibs to girlfriend goldie provide textbook proof of the dangers of lying . goldie hawn's touching kook toni simmons certainly deserved to win her oscar . ingrid bergman's work as the stiff-as-starch nurse stephanie is also touching to watch as she comes out of her shell , slowly and nervously . this is a great movie to watch in the springtime , or any time for that matter . it's very underrated ; i never heard about it until i found it in the video store , and what a find !\u00bb\n",
        "\n",
        "MEDIAN (84045, 0.33689069747924805): \u00abembarrassingly bad , low-budget italian-made war movie set in holland in the dying days of wwii . a tedious , plodding storyline concerning a plot to steal some diamonds from a german hq , awful acting and dreadful editing make this movie a prize turkey from the opening scene right through to the cringeworthy oh-so-60s `romantic' ending which will have you reaching for the puke bucket - that is if you haven't already reached for the `off' button long before . the worst performances come from john ireland as captain o'connor and the blonde female lead , whose name escapes me . she plays the `love interest' to our rugged leading man . perhaps it wasn't entirely her fault , as back then female romantic leads , especially in action movies , were often written as weak , wishy-washy , sobbing , super-sensitive emotional jellyfish . this one is no exception , and the second , supporting female character is just as bad . simply nauseating to watch . even the action scenes ( which are few and far between , except towards the end ) are boring and predictable . most ludicrously , in the climatic battle , we have rugged leading man and his two mates holed up in some sandbagged bunker and effortlessly gunning down endless attacking germans right , left and centre . the germans of course are all terrible shots and even seem to be eager cannon-fodder , as they make little or no effort to take cover , dying spectacularly in droves amid much flailing of arms and comic-book shouts of `aaaaargh' . this is glorification of war at its very worst . then suddenly - right in the middle of the battle - the resistance guy pulls up completely unmolested in a stolen german jeep and trots effortlessly along a convenient trench - which leads directly into the bunker and which somehow hundreds of germans approaching from all sides have failed to spot - and calmly joins our heroes inside the bunker . another scene of crass stupidity that really must be seen to be believed has captain o'connor flying over the german lines in a reconnaissance plane which , with the help of some clumsily-inserted old newsreel footage , is suddenly and miraculously transformed into a heavy bomber disgorging its massive payload from wide-open bomb bays and pulverising the germans beneath , before once more instantaneously reverting to being a small reconnaissance plane again . the concept of an ongoing truce between the resistance fighters and the occupying german army also seems ludicrous to me , yet this is a central theme of the movie . the english title of the film was obviously inspired by `the dirty dozen' ( which was made around the same time ) but it doesn't deserve to be mentioned in the same breath . the original italian title of this film ( dalle ardenne all' inferno ) , the sleeve notes for the english language video release are also grossly misleading . this film has absolutely nothing to do with the battle of the ardennes . the ardennes isn't even in holland - it's a part of belgium - which indicates that the film-makers' knowledge of world war ii events and geography was just as limited as their ability to make even a half-decent film . don't waste two hours of your valuable time on this rubbish . one of the worst movies i've had the misfortune to sit through - and i've sat through some garbage ! !\u00bb\n",
        "\n",
        "LEAST (44173, -0.15633562207221985): \u00abfrom what i understand , fox was embarrassed they released a pg-13 alien/predator movie not so long ago . it was not well received by any means . not exactly sure where to go next , seeing as they thought anderson was the best director for the franchise and they had produced a true sci-fi gem , fox turned to it's small , but knowledgeable group of monkeys for answers . these monkeys were by no means veterans of writing sci-fi flicks , but had seen burton's planet of the apes remake and house of the dead . their first task : hire actors . fox gave them a reasonable budget but the monkeys wanted to save the money . they hired fifteen tv actors shortly after . now , the script . the monkeys wanted to save more of the budget so they wrote the movie themselves . leaving out important aspects of the two franchises was the easy part . thinking of great new lines for the general audience to remember years down the line - that was more difficult . they butted heads awhile and came up with a truly award-winning screenplay equipped with clich\u00e9 characters , idiotic decisions an gaping plot holes . fox was pleased thus far with the results but wanted to see what was to become of the centerpieces to the film - the aliens and predators . the monkeys again wanted to save money in the budget so they decided to trash the great robotics used in the otherwise terrible avp original and go with the man-in-the-suit alien seen in the old films . the actors playing the aliens had trouble fitting into the suits as they weren't properly sized by the monkeys so they jiggled their plastic heads throughout the film with honor . as for the predators , the monkeys decided one predator was enough this time around ( again , saving budget ) to fight the hordes of aliens that seemingly come out of nowhere . but what about the effects , you ask ? come on now , people . they may be monkeys but they clearly knew cgi would play a key role in the film . without diving into the budget , the monkeys used a standard final cut program and cut and pasted some very nice fire and spark effects throughout . putting red and green filters over the camera lens provided some excellent predator visions . the setting was something the monkeys thought long and hard about . if this was to be on earth , in colorado of all places , they needed to make it realistic . this was where they admitted they might have made a mistake . see , the monkeys didn't have proper training in this department so they thought turning the lights off in the city and having the movie play out in the dead of night and in the rain was the right thing to do . they simply forgot people like to see the creatures instead of looking at shadows and rain the whole film . to add insult to injury , the monkeys accidentally filmed all the fight scenes incredibly close so no one could see what was fighting or who it was . but again , rookie mistake . the rating . fox told the monkeys to make the movie r-rated . that was easy . without showing how many of the injuries or deaths actually happened , the monkeys made a habit of showing the carnage after the fact . it was simple : the viewers got the gore they desired and the monkeys didn't have to film the majority of action shots involving that violence . some of the actors originally had questions concerning the screenplay . why does a blue liquid the predator has endless amounts of magically disintegrate whatever he wants it to and nothing more than that ? why is an ex-convict driving around in a police car the entire movie ? why did the monkeys forget to show a full body shot of the aliens ? why does a clock play a more memorable role than any of the main characters ? the list of questions just kept growing but the monkeys ignored them and finished their masterpiece . fox was thrilled with their work . so thrilled that they opened the movie nationwide on christmas day and even spent a few bucks advertising it the week before it came out . the monkeys had successfully made another installment in these cherished franchises . but some ask , what ever happened to the budget the monkeys forgot to use ? they put it towards their next film : aliens vs . predator vs . hulk hogan . they knew the general public would be upset with the title but they have since released this statement : \" to the people- do not worry about our upcoming film . it will be rated r and will have violence . \" and everyone lived happily ever after . the end .\u00bb\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Do the word vectors show useful similarities?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_models = simple_models[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "from IPython.display import HTML\n",
      "# pick a random word with a suitable number of occurences\n",
      "while True:\n",
      "    word = random.choice(word_models[0].index2word)\n",
      "    if word_models[0].vocab[word].count > 10:\n",
      "        break\n",
      "# or just pick a word from the relevant domain:\n",
      "# word = 'plot'\n",
      "similars_per_model = [str(model.most_similar(word,topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
      "similar_table = (\"<table><tr><th>\" +\n",
      "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
      "    \"</th></tr><tr><td>\" +\n",
      "    \"</td><td>\".join(similars_per_model) +\n",
      "    \"</td></tr></table>\")\n",
      "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].vocab[word].count))\n",
      "HTML(similar_table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "most similar words for 'comedy/drama' (38 occurences)\n"
       ]
      },
      {
       "html": [
        "<table><tr><th>Doc2Vec(dm/c,d100,n5,w5,mc2,t4)</th><th>Doc2Vec(dbow,d100,n5,mc2,t4)</th><th>Doc2Vec(dm/m,d100,n5,w10,mc2,t4)</th></tr><tr><td>[('comedy', 0.7096928358078003),<br>\n",
        "('drama', 0.6825233101844788),<br>\n",
        "('dramedy', 0.6664647459983826),<br>\n",
        "('thriller', 0.6615678071975708),<br>\n",
        "('horror/comedy', 0.6410363912582397),<br>\n",
        "('adventure', 0.6175029277801514),<br>\n",
        "('chiller', 0.5992485284805298),<br>\n",
        "('melodrama', 0.5929774045944214),<br>\n",
        "('romance', 0.576662540435791),<br>\n",
        "('romp', 0.5749073028564453),<br>\n",
        "('science-fiction', 0.5690299868583679),<br>\n",
        "('farce', 0.5652514100074768),<br>\n",
        "('weeper', 0.5628592371940613),<br>\n",
        "('drama/comedy', 0.5627389550209045),<br>\n",
        "('whodunit', 0.5624251961708069),<br>\n",
        "('sci-fi', 0.5603950023651123),<br>\n",
        "('mockumentary', 0.5558925271034241),<br>\n",
        "('biopic', 0.5510786771774292),<br>\n",
        "('sitcom', 0.5482240915298462),<br>\n",
        "('road-movie', 0.5472671985626221)]</td><td>[('adrenaline-pumping', 0.45663613080978394),<br>\n",
        "('kipling', 0.4251996576786041),<br>\n",
        "('appears', 0.4016043245792389),<br>\n",
        "('five-second', 0.3902825713157654),<br>\n",
        "(\"fmv's\", 0.3895858824253082),<br>\n",
        "('aardvarks', 0.3821781873703003),<br>\n",
        "('promulgated', 0.3801535367965698),<br>\n",
        "('inert', 0.37795290350914),<br>\n",
        "('floorboards', 0.37393927574157715),<br>\n",
        "(\"skeletor's\", 0.37129074335098267),<br>\n",
        "('generate', 0.36898282170295715),<br>\n",
        "('open-ended', 0.36304017901420593),<br>\n",
        "('`i', 0.36297476291656494),<br>\n",
        "('inching', 0.3623065948486328),<br>\n",
        "('digestive', 0.36074209213256836),<br>\n",
        "('yoji', 0.36062514781951904),<br>\n",
        "('bergman', 0.36027780175209045),<br>\n",
        "(\"hodder's\", 0.35910874605178833),<br>\n",
        "('40-something', 0.355363667011261),<br>\n",
        "('bushido', 0.35415762662887573)]</td><td>[('comedy-drama', 0.6464394330978394),<br>\n",
        "('thriller', 0.6074070930480957),<br>\n",
        "('comedy', 0.597672700881958),<br>\n",
        "('dramedy', 0.5864953994750977),<br>\n",
        "('action-drama', 0.5831291079521179),<br>\n",
        "('actioner', 0.5727459192276001),<br>\n",
        "('potboiler', 0.5611938238143921),<br>\n",
        "('road-movie', 0.5596767663955688),<br>\n",
        "('weeper', 0.5421388149261475),<br>\n",
        "('romcom', 0.5401057004928589),<br>\n",
        "('chiller', 0.5400895476341248),<br>\n",
        "('drama', 0.5357507467269897),<br>\n",
        "('flick', 0.5278905630111694),<br>\n",
        "('action/thriller', 0.5270429253578186),<br>\n",
        "('diversion', 0.5243188738822937),<br>\n",
        "('action-comedy', 0.5170137882232666),<br>\n",
        "('confection', 0.5149624347686768),<br>\n",
        "('telemovie', 0.5138946771621704),<br>\n",
        "('yarn', 0.5052254796028137),<br>\n",
        "('farce', 0.5036333799362183)]</td></tr></table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "<IPython.core.display.HTML at 0x165b0d358>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors \u2013 they remain at their random initialized values \u2013 unless you ask with the `dbow_words=1` initialization parameter. Concurrent word-training slows DBOW mode significantly, and offers little improvement (and sometimes a little worsening) of the error rate on this IMDB sentiment-prediction task. \n",
      "\n",
      "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Are the word vectors from this dataset any good at analogies?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# assuming something like\n",
      "# https://word2vec.googlecode.com/svn/trunk/questions-words.txt \n",
      "# is in local directory\n",
      "# note: this takes many minutes\n",
      "for model in word_models:\n",
      "    sections = model.accuracy('questions-words.txt')\n",
      "    correct, incorrect = (len(sum((s['correct'] for s in sections), [])), len(sum((s['incorrect'] for s in sections),[])))\n",
      "    print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Doc2Vec(dm/c,d100,n5,w5,mc2,t4): 28.13% correct (5650 of 20086)\n",
        "Doc2Vec(dbow,d100,n5,mc2,t4): 0.01% correct (2 of 20086)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Doc2Vec(dm/m,d100,n5,w10,mc2,t4): 27.49% correct (5522 of 20086)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even though this is a tiny, domain-specific dataset, it shows some meagher capability on the general word analogies \u2013 at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Slop"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "This cell left intentionally erroneous. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To mix the Google dataset (if locally available) into the word tests..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Word2Vec\n",
      "w2v_g100b = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
      "w2v_g100b.compact_name = 'w2v_g100b'\n",
      "word_models.append(w2v_g100b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get copious logging output from above steps..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "rootLogger = logging.getLogger()\n",
      "rootLogger.setLevel(logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To auto-reload python code while developing..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
